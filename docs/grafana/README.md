# Grafana Dashboards

## Overview

The zfs_exporter ships three Grafana dashboards covering ZFS pool health,
dataset usage, service status, and anomaly detection. The generated JSON files
live in `contrib/grafana/data/`:

| File                | UID            | Purpose                                         |
| ------------------- | -------------- | ----------------------------------------------- |
| `zfs-status.json`   | `zfs-status`   | NOC-screen stat panels for at-a-glance health   |
| `zfs-details.json`  | `zfs-details`  | Drill-down graphs and tables for investigation  |
| `zfs-combined.json` | `zfs-combined` | Compact stat header + collapsed drill-down rows |

All dashboards require a **Prometheus** datasource containing metrics scraped
from the zfs_exporter.

## Directory Layout

```
contrib/grafana/
├── data/                        # generated by `make dashboards`
│   ├── zfs-combined.json
│   ├── zfs-details.json
│   └── zfs-status.json
├── kustomization.yaml           # Kustomize: ConfigMaps + GrafanaDashboard CRs
├── zfs-combined.yaml            # GrafanaDashboard CR
├── zfs-details.yaml             # GrafanaDashboard CR
└── zfs-status.yaml              # GrafanaDashboard CR
```

## Installation

### Kubernetes (Kustomize + Grafana Operator)

If you run the [Grafana Operator](https://grafana.github.io/grafana-operator/),
deploy the dashboards with Kustomize:

```bash
kubectl apply -k contrib/grafana/
```

This creates:

- A **ConfigMap** per dashboard (wrapping the JSON from `data/`)
- A **GrafanaDashboard** CR per dashboard that references its ConfigMap

The CRs target Grafana instances with the label `app: grafana` and place
dashboards in the **Infrastructure** folder. Edit the `instanceSelector` in the
YAML files to match your Grafana deployment labels.

The datasource is mapped from `DS_PROMETHEUS` to a datasource named
`Prometheus`. Update the `datasources` section in each CR if your datasource
has a different name.

### UI Import

1. Open Grafana and navigate to **Dashboards > Import**.
2. Click **Upload JSON file** and select one of the files from
   `contrib/grafana/data/`.
3. Select your Prometheus datasource when prompted.
4. Click **Import**.

### File Provisioning

Add a provisioning config to `/etc/grafana/provisioning/dashboards/`:

```yaml
apiVersion: 1
providers:
  - name: zfs
    type: file
    options:
      path: /path/to/contrib/grafana/data
      foldersFromFilesStructure: false
```

Restart Grafana after adding the config. Dashboards will be created
automatically and kept in sync with the JSON files on disk.

### Datasource Requirement

Each dashboard uses a template variable named `datasource` of type `prometheus`.
On first load Grafana will prompt you to select a Prometheus datasource. If you
have only one Prometheus datasource it will be selected automatically.

## Dashboard: ZFS Status

**File:** `zfs-status.json` | **UID:** `zfs-status`

A NOC-screen overview designed for wall displays and at-a-glance health checks.
All panels are stat panels with color-coded backgrounds (green/yellow/red).

### Pool Health Row

| Panel                 | Metric                                                            | Description                                                                                                                                 |
| --------------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| Pool Health           | `zfs_pool_health{state="online"}`                                 | Shows ONLINE (green) or NOT ONLINE (red) per pool                                                                                           |
| Pool Capacity         | `zfs_pool_allocated_bytes / zfs_pool_size_bytes`                  | Capacity percentage. Green < 80%, yellow 80-90%, red > 90%                                                                                  |
| Resilver/Scrub Status | `zfs_pool_resilver_active`, `zfs_pool_scrub_active`               | IDLE (green) or ACTIVE (orange) for resilver/scrub operations                                                                               |
| Pool Days Until Full  | `zfs_pool_free_bytes / (-deriv(zfs_pool_free_bytes[7d])) / 86400` | Estimated days until pool fills based on 7-day trend. Red < 7d, yellow 7-30d, green > 30d. Shows "Not filling" when the pool is not growing |

### Service Health Row

| Panel              | Metric                                                                     | Description                                                                                  |
| ------------------ | -------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| Service Status     | `zfs_service_up`                                                           | UP (green) or DOWN (red) for each monitored systemd service (ZFS, NFS, SMB, iSCSI)           |
| NFS Share Mismatch | `count(zfs_dataset_share_nfs == 1) and zfs_service_up{service="nfs"} == 0` | OK (green) when consistent; MISMATCH (red) when NFS shares exist but the NFS service is down |
| SMB Share Mismatch | `count(zfs_dataset_share_smb == 1) and zfs_service_up{service="smb"} == 0` | OK (green) when consistent; MISMATCH (red) when SMB shares exist but the SMB service is down |
| Exporter Up        | `zfs_up`                                                                   | UP (green) when the exporter can execute ZFS commands; DOWN (red) otherwise                  |

**When to use:** Mount this on a wall display or use as your first stop when
investigating a ZFS alert. It answers "is everything healthy?" at a glance.

## Dashboard: ZFS Details

**File:** `zfs-details.json` | **UID:** `zfs-details`

Expanded rows with drill-down graphs and tables for investigating issues
identified on the Status dashboard.

### Pool Capacity Row

| Panel                            | Type                    | Description                                                                                                |
| -------------------------------- | ----------------------- | ---------------------------------------------------------------------------------------------------------- |
| Pool Usage Over Time             | Stacked area timeseries | Allocated and free bytes over time per pool                                                                |
| Pool Usage % (Allocated / Total) | Horizontal bar gauge    | Current capacity percentage per pool. Color gradient: green < 70%, yellow 70-80%, orange 80-90%, red > 90% |
| Fragmentation Over Time          | Timeseries              | Pool fragmentation ratio over time. Threshold line at 50% marks performance risk zone                      |

### Dataset Usage Row

| Panel                      | Type                   | Description                                                                    |
| -------------------------- | ---------------------- | ------------------------------------------------------------------------------ |
| Top Datasets by Used Space | Table with gauge cells | Top 25 datasets ranked by used bytes. Yellow > 100 GiB, red > 1 TiB           |
| Dataset Available Space    | Table with gauge cells | Available space per dataset. Red < 10 GiB, yellow 10-100 GiB, green > 100 GiB |
| Dataset Usage Over Time    | Timeseries             | Used bytes per dataset over time                                               |

### Per-Service Rows (NFS, SMB, iSCSI)

Each service gets a collapsed row containing:

| Panel                               | Type              | Description                                                   |
| ----------------------------------- | ----------------- | ------------------------------------------------------------- |
| {Service} Service                   | Stat              | UP/DOWN status for the individual service                     |
| {Service} Shared Datasets / Volumes | Table             | Datasets shared via the service (or zvol inventory for iSCSI) |
| {Service} Service Timeline          | Timeseries (step) | Service up/down history. 1 = running, 0 = down               |

### Anomaly Detection Row

| Panel                                       | Type       | Description                                                                                                                                                                                                                                       |
| ------------------------------------------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Dataset Daily Growth Rate                   | Timeseries | Estimated daily growth rate per dataset derived from the 1-hour derivative of used bytes                                                                                                                                                          |
| Datasets Outside Normal Range (7d Baseline) | Table      | Datasets whose current usage deviates more than 2 standard deviations from their 7-day average. Columns: dataset, pool, Current, 7d Avg, Std Dev, Deviation, Sigma. **Requires recording rules** (see [Prometheus docs](../prometheus/README.md)) |
| Pool Days Until Full (7d Trend)             | Timeseries | Predicted days until each pool fills based on 7-day linear extrapolation. Threshold zones: red < 7d, orange 7-30d, yellow 30-90d                                                                                                                 |

**When to use:** After the Status dashboard flags a problem, switch here to
investigate root cause with detailed time-series data and tables.

## Dashboard: ZFS Combined

**File:** `zfs-combined.json` | **UID:** `zfs-combined`

An all-in-one dashboard combining a compact stat header with collapsed
drill-down rows. Ideal for teams that want a single dashboard.

### Top Stats (6 panels)

Six stat panels across the top, each 4 grid units wide:

1. **Pool Health** - ONLINE/NOT ONLINE per pool
2. **Pool Capacity** - Capacity percentage with green/yellow/red thresholds
3. **Service Status** - All monitored services UP/DOWN
4. **Resilver/Scrub Status** - IDLE/ACTIVE for resilver and scrub
5. **Pool Days Until Full** - Estimated days with trend-based prediction
6. **Exporter Up** - ZFS command execution health

### Collapsed Rows

Click any row header to expand:

| Row               | Contents                                                                |
| ----------------- | ----------------------------------------------------------------------- |
| Pool Details      | Pool Usage Over Time, Pool Usage % bar gauge, Fragmentation             |
| Dataset Details   | Top Datasets table, Available Space table, Dataset Usage Over Time      |
| NFS               | NFS Service stat, NFS Shared Datasets table, NFS Service Timeline       |
| SMB               | SMB Service stat, SMB Shared Datasets table, SMB Service Timeline       |
| iSCSI             | iSCSI Service stat, iSCSI Volumes (zvols) table, iSCSI Service Timeline |
| Anomaly Detection | Growth Rate, 7d Deviation Table, Pool Fill Prediction                   |

**When to use:** Use as a single-pane-of-glass when you don't want to switch
between Status and Details dashboards.

## Variables

All three dashboards define two template variables:

| Variable     | Type                    | Description                                                                                                                          |
| ------------ | ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| `datasource` | Datasource (Prometheus) | Selects which Prometheus datasource to query. Shown as "Data Source" in the dashboard controls                                       |
| `pool`       | Query                   | Populated from `label_values(zfs_pool_size_bytes, pool)`. Multi-select with "All" option. Filters all panels to the selected pool(s) |

## Troubleshooting

### Panels show "No data"

1. Verify the `datasource` variable points to the correct Prometheus instance.
2. Check that the zfs_exporter scrape job is configured in Prometheus and the
   `job` label matches (default: `zfs_exporter`).
3. Confirm the `pool` variable dropdown is populated. If empty, Prometheus has
   no `zfs_pool_size_bytes` metrics. Check that the exporter is running and
   being scraped.
4. Try querying `zfs_pool_health` directly in Prometheus to verify data exists.

### Deviation table shows NaN

The "Datasets Outside Normal Range" table depends on recording rules that
pre-compute 7-day averages and standard deviations. If you see NaN values:

1. Ensure the recording rules are loaded in Prometheus. See
   [Prometheus Recording Rules](../prometheus/README.md#recording-rules).
2. The `zfs:dataset_used_bytes:avg7d` rule requires 7 days of scrape history
   before it produces results. Until then, values will be NaN.
3. Verify rules are evaluating by querying `zfs:dataset_used_bytes:avg7d` in
   Prometheus. If it returns no results, the rules are not loaded.

### Days Until Full shows "Not filling"

This is expected behavior. When a pool's free space is not decreasing (or is
increasing), the linear prediction produces a negative value, which is mapped to
"Not filling" with a green background. No action needed.

### Panels appear too wide or overlap

The dashboard JSON is generated by the dashgen tool. If panel layout looks wrong
after manual edits, regenerate with:

```bash
make dashboards
```

This rebuilds all three dashboards from the Go source of truth.

### Customizing services

The default service list is NFS, SMB, and iSCSI. To change which services appear
in dashboards:

1. Edit `tools/dashgen/config.go` and modify the `DefaultConfig.Services` slice.
2. Run `make dashboards` to regenerate all JSON files.
3. Re-import or re-provision the dashboards in Grafana.
